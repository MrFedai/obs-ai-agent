import streamlit as st
from langchain_chroma import Chroma
from langchain_ollama import OllamaEmbeddings, ChatOllama
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

# Sayfa AyarlarÄ±
st.set_page_config(page_title="OBS AI Asistan", layout="wide") # Layout'u wide yaptÄ±m ki debug paneli rahat gÃ¶rÃ¼nsÃ¼n
st.title("ğŸ“ FakÃ¼lte AI AsistanÄ±")

# Model ve DB Kurulumu
@st.cache_resource
def init_rag():
    # 1. Embedding ve VeritabanÄ± BaÄŸlantÄ±sÄ±
    embedding = OllamaEmbeddings(model="nomic-embed-text")
    vector_store = Chroma(persist_directory="./chroma_db", embedding_function=embedding)
    
    # 2. Retriever (Getirici) AyarÄ± - En alakalÄ± 5 parÃ§a
    retriever = vector_store.as_retriever(search_kwargs={"k": 5}) 
    
    # 3. LLM (Beyin) AyarÄ±
    llm = ChatOllama(model="llama3.1", temperature=0) # HalÃ¼sinasyonu Ã¶nlemek iÃ§in 0
    
    # 4. Prompt Åablonu
    template = """
    Sen Ã¼niversite Ã¶ÄŸrencilerinin sorularÄ±nÄ± yanÄ±tlayan yardÄ±msever bir asistanÄ±sÄ±n.
    AÅŸaÄŸÄ±da verilen baÄŸlam (Context) bilgilerini kullanarak soruyu TÃ¼rkÃ§e cevapla.
    
    EÄŸer baÄŸlamda sorunun cevabÄ± yoksa, dÃ¼rÃ¼stÃ§e "Verilen dÃ¶kÃ¼manlarda bu bilgi yer almÄ±yor" de.
    Uydurma cevap verme.
    
    BaÄŸlam: {context}
    
    Soru: {question}
    """
    prompt = ChatPromptTemplate.from_template(template)
    
    return retriever, prompt, llm

# Sistemi baÅŸlat
retriever, prompt, llm = init_rag()

# --- SOHTBET GEÃ‡MÄ°ÅÄ° BAÅLATMA (HATA BURADAYDI) ---
if "messages" not in st.session_state:
    st.session_state.messages = []

# Eski mesajlarÄ± ekrana Ã§iz
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# --- YENÄ° MESAJ MANTIÄI ---
if prompt_input := st.chat_input("Sorunuzu yazÄ±n..."):
    # 1. KullanÄ±cÄ± mesajÄ±nÄ± ekle ve gÃ¶ster
    st.session_state.messages.append({"role": "user", "content": prompt_input})
    with st.chat_message("user"):
        st.markdown(prompt_input)

    # 2. AI CevabÄ±nÄ± Ãœret
    with st.chat_message("assistant"):
        status_container = st.status("DÃ¶kÃ¼manlar taranÄ±yor...", expanded=False)
        
        try:
            # A. DÃ¶kÃ¼manlarÄ± Bul
            docs = retriever.invoke(prompt_input)
            
            # B. Yan Panele (Sidebar) KanÄ±tlarÄ± YazdÄ±r (DEBUG MODU)
            with st.sidebar:
                st.header("ğŸ” Modelin GÃ¶zÃ¼")
                st.write(f"**Soru:** {prompt_input}")
                st.divider()
                if not docs:
                    st.error("âŒ VeritabanÄ±nda alakalÄ± kayÄ±t bulunamadÄ±.")
                
                for i, doc in enumerate(docs):
                    with st.expander(f"ğŸ“„ KanÄ±t {i+1} (Kaynak: {doc.metadata.get('source', 'Bilinmiyor')})"):
                        st.caption(f"Sayfa: {doc.metadata.get('page', '-')}")
                        st.info(doc.page_content) # Ä°Ã§eriÄŸi gÃ¶ster
            
            # C. CevabÄ± Ãœret
            context_text = "\n\n".join([d.page_content for d in docs])
            chain = prompt | llm | StrOutputParser()
            
            response = chain.invoke({"context": context_text, "question": prompt_input})
            
            status_container.update(label="Cevap hazÄ±r!", state="complete", expanded=False)
            st.markdown(response)
            
            # CevabÄ± geÃ§miÅŸe kaydet
            st.session_state.messages.append({"role": "assistant", "content": response})
            
        except Exception as e:
            st.error(f"Bir hata oluÅŸtu: {e}")
